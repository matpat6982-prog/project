# -*- coding: utf-8 -*-
"""churn 2

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10zwDaMWp_E1TpSsyscRdm3YT8_YK-RxZ

Here is the corrected, fully integrated **Google Colab Notebook**.

I have aligned this strictly with the **methods found in your uploaded source files**:

* **Outlier Detection**: Switched to **Isolation Forest** (found in `Outleir detection.ipynb`) instead of the basic IQR method.
* **Ensemble**: Switched to **Balanced Random Forest** (found in `Balancing techniques.ipynb`), which is more advanced than a standard Random Forest for imbalanced data.
* **Balancing**: Used **SMOTE** (found in `Balancing techniques.ipynb`) specifically for the SVM model.
* **Scaling**: Used **StandardScaler** (found in `Preprocessing.ipynb`).

You can copy these cells directly into Colab.

---

### **Step 1: Setup & Data Loading**

We import the specific libraries referenced in your source documents and load the Telco dataset.
"""

# @title 1. Import Libraries & Load Data
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# -- From 'Preprocessing.ipynb' & 'svm- with breast cancer (1).ipynb' --
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import classification_report, confusion_matrix

# -- From 'Outleir detection.ipynb' --
from sklearn.ensemble import IsolationForest

# -- From 'Balancing techniques.ipynb' --
from imblearn.over_sampling import SMOTE
from imblearn.ensemble import BalancedRandomForestClassifier

# -- For Clustering (Standard Advanced ML approach) --
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA

# Load the dataset
try:
    df = pd.read_csv('WA_Fn-UseC_-Telco-Customer-Churn.csv')
    print("Dataset loaded successfully.")
except FileNotFoundError:
    print("Error: Please upload 'WA_Fn-UseC_-Telco-Customer-Churn.csv' to Colab files.")

# Data Cleaning (Handling non-numeric TotalCharges)
df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')
df['TotalCharges'].fillna(0, inplace=True) # Fill missing values with 0
df.drop(['customerID'], axis=1, inplace=True) # ID is not needed

df.head()

"""---

### **Step 2: Preprocessing & Scaling**

We apply **StandardScaler** as demonstrated in `Preprocessing.ipynb`. We also encode categorical text into numbers so the models can read them.
"""

# @title 2. Feature Encoding & Scaling
# Separate features (X) and target (y)
X = df.drop('Churn', axis=1)
y = df['Churn'].apply(lambda x: 1 if x == 'Yes' else 0)

# Convert categorical columns to numeric (One-Hot Encoding)
X_encoded = pd.get_dummies(X, drop_first=True)

# Apply Standard Scaling (Z-Score Normalization)
# Source: Preprocessing.ipynb
scaler = StandardScaler()
X_scaled = pd.DataFrame(scaler.fit_transform(X_encoded), columns=X_encoded.columns)

print("Data Scaled. Shape:", X_scaled.shape)
X_scaled.head()

"""---

### **Step 3: Advanced Outlier Detection**

Instead of simple boxplots, we use **Isolation Forest** as seen in `Outleir detection.ipynb`. This is an unsupervised learning algorithm that identifies anomalies by isolating outliers in the data.
"""

# @title 3. Outlier Detection (Isolation Forest)
# Source: Outleir detection.ipynb

print(f"Original Row Count: {len(X_scaled)}")

# Contamination is the % of outliers we expect (e.g., 5%)
iso = IsolationForest(contamination=0.05, random_state=42)
outlier_labels = iso.fit_predict(X_scaled)

# -1 indicates an outlier, 1 indicates normal
# We keep only the normal rows (label == 1)
X_clean = X_scaled[outlier_labels == 1]
y_clean = y[outlier_labels == 1]

print(f"Row Count after removing outliers: {len(X_clean)}")
print(f"Outliers removed: {len(X_scaled) - len(X_clean)}")

"""---

### **Step 4: Clustering**

We group customers into clusters to find hidden patterns.
"""

# @title 4. Clustering (K-Means)
# We cluster the clean data into 3 groups
kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)
clusters = kmeans.fit_predict(X_clean)

# Visualize clusters using PCA (Compressing to 2D for the plot)
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_clean)

plt.figure(figsize=(10, 6))
sns.scatterplot(x=X_pca[:, 0], y=X_pca[:, 1], hue=clusters, palette='viridis', s=50)
plt.title('Customer Clusters (K-Means Visualization)')
plt.xlabel('PCA Component 1')
plt.ylabel('PCA Component 2')
plt.legend(title='Cluster')
plt.show()

"""---

### **Step 5: Ensemble Model (Balanced Random Forest)**

We use **Balanced Random Forest**, which is specifically designed for imbalanced datasets (like Churn data). This was imported in your `Balancing techniques.ipynb`.
"""

# @title 5. Ensemble Learning (Balanced Random Forest)
# Source: Balancing techniques.ipynb

# Split data
X_train, X_test, y_train, y_test = train_test_split(X_clean, y_clean, test_size=0.2, random_state=42)

# Initialize Balanced Random Forest
# This model down-samples the majority class internally during training
brf_model = BalancedRandomForestClassifier(n_estimators=100, random_state=42)
brf_model.fit(X_train, y_train)

# Predict
y_pred_brf = brf_model.predict(X_test)

# Evaluate
print("--- Balanced Random Forest Results ---")
print(classification_report(y_test, y_pred_brf))

# Confusion Matrix
plt.figure(figsize=(5, 4))
sns.heatmap(confusion_matrix(y_test, y_pred_brf), annot=True, fmt='d', cmap='Greens')
plt.title('Balanced RF Confusion Matrix')
plt.show()

"""---

### **Step 6: SVM with SMOTE**

For SVM (`svm- with breast cancer (1).ipynb`), we need to manually balance the data first. We use **SMOTE** (Synthetic Minority Over-sampling Technique) as detailed in `Balancing techniques.ipynb`.
"""

# @title 6. SVM with SMOTE Balancing
# Source: Balancing techniques.ipynb & svm- with breast cancer (1).ipynb

# 1. Apply SMOTE to training data ONLY
smote = SMOTE(random_state=42)
X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)

print(f"Training data shape before SMOTE: {y_train.value_counts().to_dict()}")
print(f"Training data shape after SMOTE: {y_train_smote.value_counts().to_dict()}")

# 2. Train SVM
# RBF kernel is standard for complex non-linear data
svm_model = SVC(kernel='rbf', C=1.0, random_state=42)
svm_model.fit(X_train_smote, y_train_smote)

# 3. Predict
y_pred_svm = svm_model.predict(X_test)

# 4. Evaluate
print("\n--- SVM Results ---")
print(classification_report(y_test, y_pred_svm))

# Confusion Matrix
plt.figure(figsize=(5, 4))
sns.heatmap(confusion_matrix(y_test, y_pred_svm), annot=True, fmt='d', cmap='Blues')
plt.title('SVM Confusion Matrix')
plt.show()